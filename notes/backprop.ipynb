{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5fa88d8-a5f7-4dc2-95b2-88582c1017dd",
   "metadata": {},
   "source": [
    "---\n",
    "# Jacobian-Vector Product (JVP)\n",
    "---\n",
    "\n",
    "Let $y = f(u_1, ..., u_n)$ where $u_i = g_i(x_1, ..., x_m)$. Using vector notation, $y = f(\\mathbf{u})$ where $\\mathbf{u} = \\mathbf{g}(\\mathbf{x})$. The gradient $\\nabla_x y$ is the vector of partial derivatives $\\frac{\\partial y}{\\partial x_i}$ for $i=1, 2, \\dots, m$. Since $y$ depends on $x_i$ through $\\mathbf{u}$,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x_i} = \\frac{\\partial y}{\\partial u_1}\\frac{\\partial u_1}{\\partial x_i} + \\dots + \\frac{\\partial y}{\\partial u_n}\\frac{\\partial u_n}{\\partial x_i} = \\sum_{j=1}^n \\frac{\\partial y}{\\partial u_j}\\frac{\\partial u_j}{\\partial x_i}\n",
    "$$\n",
    "\n",
    "by the Chain Rule. In other words, the $i$-th component $\\frac{\\partial y}{\\partial x_i}$ of the gradient $\\nabla_x y$ is the total rate of change of $y$ wrt $x_i$. Notice the above equation can be expressed as a dot product. More precisely,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x_i} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial y}{\\partial u_1} & \\dots & \\frac{\\partial y}{\\partial u_n}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial u_1}{\\partial x_i} & \\dots & \\frac{\\partial u_n}{\\partial x_i}\n",
    "\\end{bmatrix}^T\n",
    "$$\n",
    "\n",
    "Thus, $\\nabla_x y$ is obtained via matrix multiplication,\n",
    "\n",
    "$$\n",
    "\\nabla_x y = \\begin{bmatrix}\n",
    "\\frac{\\partial u_1}{\\partial x_1} & \\dots & \\frac{\\partial u_1}{\\partial x_m} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial u_n}{\\partial x_1} & \\dots & \\frac{\\partial u_n}{\\partial x_m} \\\\\n",
    "\\end{bmatrix}^T\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial y}{\\partial u_1} \\\\ \n",
    "\\vdots \\\\\n",
    "\\frac{\\partial y}{\\partial u_n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The last expression can be written as $\\nabla_x y = J^T_{\\mathbf{u} \\rightarrow {x}} \\nabla_u y$ where $J_{\\mathbf{u} \\rightarrow {x}} \\in \\mathbb{R}^{n \\times m}$. \n",
    "\n",
    "**Conclusion:** The gradient $\\nabla_x y$ is a matrix-vector product. More specifically, a Jacobian-vector product.\n",
    "\n",
    "## Extending to more nested functions\n",
    "\n",
    "Next, we explore how the chain rule behaves when considering more layers of function dependencies, i.e., more nested functions. In this regard,\n",
    "let \n",
    "\n",
    "\\begin{align}\n",
    "y = f(\\mathbf{u}) \\\\\n",
    "\\mathbf{u} = g(\\mathbf{x}) \\\\\n",
    "\\mathbf{x} = h(\\mathbf{t}) \\\\\n",
    "\\end{align}\n",
    "\n",
    "Said differently, $y$ holds the output of the composition $f \\circ g \\circ h$ evaluated at $\\mathbf{t}$, i.e., $y = (f \\circ g \\circ h)(\\mathbf{t})$. We are interested in computing $\\nabla_t y$, the gradient of $y$ with respect to $\\mathbf{t}$, given by\n",
    "\n",
    "$$\n",
    "\\nabla_t y =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial y}{\\partial t_1} & \\dots & \\frac{\\partial y}{\\partial t_l}\n",
    "\\end{bmatrix}^T \\in \\mathbb{R}^{l \\times 1}\n",
    "$$\n",
    "\n",
    "Let us first solve for the $k$-th component $\\frac{\\partial y}{\\partial t_k}$ where $k = 1, \\dots, l$. Applying the Chain Rule twice, we get\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial t_k} = \\sum_{i=1}^n\\sum_{j=1}^m \\frac{\\partial y}{\\partial u_i} \\frac{\\partial u_i}{\\partial x_j} \\frac{\\partial x_j}{\\partial t_k}\n",
    "$$\n",
    "\n",
    "Notice we use the same convention as before were $\\mathbf{u} = (u_1, ..., u_n)$ and $\\mathbf{x} = (x_1, ..., x_m)$. We can expand the outer summation and express $\\frac{\\partial y}{\\partial t_k}$ as \n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial t_k} = \\frac{\\partial y}{\\partial u_1} \\sum_{j=1}^m \\frac{\\partial u_1}{\\partial x_j} \\frac{\\partial x_j}{\\partial t_k} + \\frac{\\partial y}{\\partial u_2} \\sum_{j=1}^m \\frac{\\partial u_2}{\\partial x_j} \\frac{\\partial x_j}{\\partial t_k} + \\dots + \\frac{\\partial y}{\\partial u_n} \\sum_{j=1}^m \\frac{\\partial u_n}{\\partial x_j} \\frac{\\partial x_j}{\\partial t_k}\n",
    "$$\n",
    "\n",
    "Which, again, is nothing more than the inner product between two vectors $\\mathbf{v_1}$ and $\\mathbf{v_2}$ where\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{aligned}\n",
    "        \\mathbf{v_1} &= \\begin{bmatrix}\n",
    "                       \\frac{\\partial y}{\\partial u_1} & \\dots & \\frac{\\partial y}{\\partial u_n}\n",
    "                       \\end{bmatrix}^T \\\\\n",
    "        \\mathbf{v_2} &= \\begin{bmatrix}\n",
    "                       \\sum_{j=1}^m\\frac{\\partial u_1}{\\partial x_j}\\frac{\\partial x_j}{\\partial t_k} & \\dots \\sum_{j=1}^m\\frac{\\partial u_n}{\\partial x_j}\\frac{\\partial x_j}{\\partial t_k}\n",
    "                        \\end{bmatrix}^T\n",
    "    \\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Notice the first vector is just the gradient of $y$ wrt $\\mathbf{u}$, i.e, $\\nabla_u y$, while the second vector is itself a collection of dot products between first partial derivatives. From the previous case, we know this second vector can be expressed as a Jacobian-vector multiplication. More precisely,\n",
    "\n",
    "$$\n",
    "\\mathbf{v_2} = \\begin{bmatrix}\n",
    "\\frac{\\partial u_1}{\\partial x_1} & \\dots & \\frac{\\partial u_1}{\\partial x_m} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial u_n}{\\partial x_1} & \\dots & \\frac{\\partial u_n}{\\partial x_m} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial x_1}{\\partial t_k} \\\\ \n",
    "\\vdots \\\\\n",
    "\\frac{\\partial x_m}{\\partial t_k}\n",
    "\\end{bmatrix} =\n",
    "J_{\\mathbf{u} \\rightarrow \\mathbf{x}} \\begin{bmatrix}\n",
    "\\frac{\\partial x_1}{\\partial t_k} \\\\ \n",
    "\\vdots \\\\\n",
    "\\frac{\\partial x_m}{\\partial t_k}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Thus, $\\frac{\\partial y}{\\partial t_k}$ becomes \n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial t_k} = \\nabla^T_u y \\cdot J_{\\mathbf{u} \\rightarrow \\mathbf{x}} \\cdot \\begin{bmatrix}\n",
    "\\frac{\\partial x_1}{\\partial t_k} \\\\ \n",
    "\\vdots \\\\\n",
    "\\frac{\\partial x_m}{\\partial t_k}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We can instead aim for all $\\frac{\\partial y}{\\partial t_k}, k=1, \\dots, l$ and compute $\\nabla_x y$ at once through\n",
    "\n",
    "$$\n",
    "\\nabla_x^T y = \\nabla^T_u y \\cdot J_{\\mathbf{u} \\rightarrow \\mathbf{x}} \\cdot J_{\\mathbf{x} \\rightarrow \\mathbf{t}}\n",
    "$$\n",
    "\n",
    "\n",
    "where $J_{\\mathbf{x} \\rightarrow \\mathbf{t}} \\in \\mathbb{R}^{m \\times l}$. After transposing,\n",
    "\n",
    "$$\n",
    "\\nabla_x y = J^T_{\\mathbf{x} \\rightarrow \\mathbf{t}} \\cdot J^T_{\\mathbf{u} \\rightarrow \\mathbf{x}} \\cdot \\nabla_u y\n",
    "$$\n",
    "\n",
    "**Conclusion:** For nested functions, the gradient propagates via successive Jacobian transposes.\n",
    "\n",
    "\n",
    "## Matrix input\n",
    "\n",
    "Consider the expression $\\mathbf{y} = f(W)$ where $f : \\mathbf{R}^{m \\times n} \\rightarrow \\mathbf{R}^n$. This could be, for example, a simple linear transformation $y = x^{\\top}W + b$ where $x \\in \\mathbf{R}^{m}$. In other words, $\\mathbf{y}$ is the projection of $x$ under $W$. To compute the jacobian of $\\mathbf{y}$ (a vector) wrt $W$ (a matrix) for this example, we first consider the derivative of a single entry from $\\mathbf{y}$ ($\\mathbf{y}_k$) wrt a single entry from $W$ ($W_{ij}$). \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{{\\partial W_{ij}}} \\mathbf{y}_k = \\frac{\\partial}{\\partial W_{ij}} \\left(x^{\\top}W[:,k]\\right) = \\frac{\\partial}{\\partial W_{ij}} \\left(x_1 W_{1k} + \\dots + x_iW_{ik} + \\dots + x_m W_{mk}\\right) = \\begin{cases} \n",
    "    x_i & j = k            \\\\\n",
    "    0   & \\text{otherwise} \\\\\n",
    "   \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "The collection of all derivatives for the scalar $\\mathbf{y}_k$ is thus a matrix of the same shape as $W$ denoted by $\\nabla_{W} y_{k}$, i.e., a gradient matrix. Also, our previous result shows that $\\nabla_{W} y_{k}$ consists of mostly $0$ entries except for the $k$th column which is equal to $x$. Explicitly, the gradient looks like\n",
    "\\begin{equation}\n",
    "\\nabla_{W}\\mathbf{y}_k = \\begin{bmatrix}\n",
    "0      & \\dots  & x_1    & \\dots  & 0      \\\\\n",
    "\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0      & \\dots  & x_m    & \\dots  & 0      \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Thus, each $y_k$ depends only on the $k$th column of $W$. This is exactly what we expect since $y_k = x^{\\top}W_{:, k}$, the dot product between $\\mathbf{x}^{\\top}$ and the $k$th column in $W$. \n",
    "\n",
    "Now, to fully describe the derivative of the vector-valued function $\\mathbf{y} \\in \\mathbf{R}^n$ with respect to the matrix $W \\in \\mathbf{R}^{m\\times n}$, we must stack the individual gradient matrices $\\nabla_W y_k \\in \\mathbf{R}^{m \\times n}$ across $k$. This results in a 3D Jacobian tensor $J \\in R^{n \\times m \\times n}$ where each slice $J[k,:,:] = \\nabla_{W} \\mathbf{y}_{k}$. Equivalently, the individual entries of the 3D tensor are given by \n",
    "\\begin{equation}\n",
    "J_{kij} = \\frac{\\partial}{\\partial W_{ij}} \\mathbf{y}_k\n",
    "\\end{equation}\n",
    "\n",
    "where, as before, we are letting the index $k$ denote the stacking dimension. For the linear example, we already derived the value of each entry, this is\n",
    "\n",
    "\\begin{equation}\n",
    "J_{kij} = \n",
    "\\begin{cases} \n",
    "    x_i & j = k            \\\\\n",
    "    0   & \\text{otherwise} \\\\\n",
    "\\end{cases}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f867ac-5d77-4dd8-bee0-a2454c8e55bc",
   "metadata": {},
   "source": [
    "### Example: Minimizing the Rosenbrock Function using gradient descent (GD)\n",
    "\n",
    "\n",
    "The Rosenbrock Function is given by\n",
    "\n",
    "$$\n",
    "y = f(u_1, u_2) = (1 - u_1)^2 + 100 (u_2 - u_1^2)^2\n",
    "$$\n",
    "\n",
    "where $u_1$ and $u_2$ are themselves functions of $x_1$ and $x_2$,\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "u_1 = g_1(x_1, x_2) = x1 + x2 \\\\\n",
    "u_2 = g_2(x_1, x_2) = x1 - x2\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "The GD algorithm iterates using the following update rule\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^{t+1} = \\mathbf{x}^{t} - \\alpha \\nabla_{\\mathbf{x}} y\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is a parameter controlling the step size of the gradient  (which, for this particular case, will be kept constant). According to our previous derivation,\n",
    "\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{x}} y   = J^T_{\\mathbf{u} \\rightarrow \\mathbf{x}} \\cdot \\nabla_u y  \\\\\n",
    "$$\n",
    "\n",
    "For the Rosenbrock function,\n",
    "\n",
    "$$\n",
    "J_{\\mathbf{u} \\rightarrow \\mathbf{x}} =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial u_1}{\\partial x_1} & \\frac{\\partial u_1}{\\partial x_2} \\\\\n",
    "\\frac{\\partial u_2}{\\partial x_1} & \\frac{\\partial u_2}{\\partial x_2} \n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "1 & 1 \\\\\n",
    "1 & -1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{u}} y = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial y}{\\partial u_1} \\\\\n",
    "\\frac{\\partial y}{\\partial u_2}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "-2(1 - u_1) - 400u_1(u_2 - u_1^2)  \\\\\n",
    "200(u_2 - u_1^2)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Hence, the gradient $\\nabla_{\\mathbf{x}} y$ becomes\n",
    "\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{x}} y = \n",
    "\\begin{bmatrix}\n",
    "-2(1 - u_1) - 400u_1(u_2 - u_1^2)  + 200(u_2 - u_1^2) \\\\\n",
    "-2(1 - u_1) - 400u_1(u_2 - u_1^2)  - 200(u_2 - u_1^2)\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad6d8cdc-295a-4687-82a3-b77ddd5a30d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dad0e39-64eb-4257-90b5-3527e36961c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(x):\n",
    "    return ((1 - u1(x)**2) + (100 * (u2(x) - u1(x)**2)**2))\n",
    "\n",
    "def u1(x):\n",
    "    return x[0] + x[1]\n",
    "\n",
    "def u2(x):\n",
    "    return x[0] - x[1]\n",
    "\n",
    "\n",
    "def grad(x):\n",
    "    return np.array(\n",
    "        [\n",
    "            (-2 * (1 - u1(x))) - (400 * u1(x) * (u2(x) - u1(x)**2)) + (200 * (u2(x) - u1(x)**2)),\n",
    "            (-2 * (1 - u1(x))) - (400 * u1(x) * (u2(x) - u1(x)**2)) - (200 * (u2(x) - u1(x)**2))\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19b74adc-5afe-4af2-9fb7-aaa77dc3b59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: [1. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Gradient descent\n",
    "# ----------------\n",
    "\n",
    "# Step function\n",
    "def step(x, alpha):\n",
    "    return x - alpha * grad(x)\n",
    "\n",
    "# Learning rate\n",
    "alpha = 1e-4\n",
    "\n",
    "# Tolerance\n",
    "tol = 1e-8\n",
    "\n",
    "# Initial guess\n",
    "x0 = np.array([0, 0])\n",
    "\n",
    "# Initial step\n",
    "x = x0\n",
    "newx = step(x, alpha)\n",
    "i = 0\n",
    "\n",
    "# Repeat until convergence\n",
    "while np.linalg.norm(newx - x) > tol:\n",
    "\n",
    "    x = newx\n",
    "    newx = step(x, alpha)\n",
    "    i += 1\n",
    "\n",
    "print(f\"Result: {x.round()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51cd6c5-824a-4df4-a8fc-21c2378fc40d",
   "metadata": {},
   "source": [
    "---\n",
    "# Neural Networks\n",
    "---\n",
    "\n",
    "Let us what we have learned about gradients of nested scalar functions and matrix inputs to derive the logic of backpropagation in neural networks. Assume a real-valued loss function $L: \\mathbb{R}^{out} \\rightarrow \\mathbb{R}$ that maps the network output values $\\mathbf{y} \\in \\mathbb{R}^{out}$ to a single scalar. The output values are themselves obtained from a non linear activation vector-valued function $\\sigma : \\mathbb{R}^{out} \\rightarrow \\mathbb{R}^{out}$ which takes the hidden projection $\\mathbf{z} = x^{\\top}W + b$ as input. These interaction are summarized as follows.\n",
    "\n",
    "\\begin{align}\n",
    "l          &= L(\\mathbf{y}) \\in \\mathbb{R}        \\\\\n",
    "\\mathbf{y} &= \\sigma(\\mathbf{z}) \\in \\mathbb{R}^{out} \\\\\n",
    "\\mathbf{z} &= \\mathbf{x}^{\\top}W + \\mathbf{b} \\in \\mathbb{R}^{out} \\\\\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf{x} \\in \\mathbb{R}^{in}$ is the input vector, $\\mathbf{b} \\in \\mathbb{R}^{out}$ the bias vector and $W \\in \\mathbb{R}^{in \\times out}$ the weight matrix. \n",
    "\n",
    "\n",
    "## Gradient matrix $\\nabla_{W}l$\n",
    "\n",
    "Our goal is to derive an expression for $\\nabla_{W}l$, the gradient matrix of $l$ wrt to $W$. We proceed by first considering the derivative wrt a single element $W_{mn}$. Since $l$ depends on $W_{mn}$ through $\\mathbf{y}$ and $\\mathbf{z}$, we have\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial l}{\\partial W_{mn}} = \\sum_{i=1}^{out} \\sum_{j=1}^{out} \\frac{\\partial l}{\\partial y_i}\\frac{\\partial y_i}{\\partial z_j}\\frac{\\partial z_j}{\\partial W_{mn}}\n",
    "\\end{equation}\n",
    "\n",
    "Simiarly than before, we can expand the outer summation and express the previous expression as follows.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial l}{\\partial W_{mn}} = \\frac{\\partial l}{\\partial y_1} \\sum_{j=1}^{out} \\frac{\\partial y_1}{\\partial z_j}\\frac{\\partial z_j}{\\partial W_{mn}} + \\dots + \\frac{\\partial l}{\\partial y_{out}} \\sum_{j=1}^{out} \\frac{\\partial y_{out}}{\\partial z_j}\\frac{\\partial z_j}{\\partial W_{mn}} = \\mathbf{u}^{\\top}\\mathbf{v}\n",
    "\\end{equation}\n",
    "\n",
    "That is, as the inner product between two vectors $\\mathbf{u}$ and $\\mathbf{v}$ in $\\mathbb{R}^{out}$ where\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{u} &= \\left[\\frac{\\partial l}{\\partial y_1}, \\dots, \\frac{\\partial l}{\\partial y_{out}}\\right]^{\\top} \\\\\n",
    "\\mathbf{v} &= \\left[\\sum_{j=1}^{out} \\frac{\\partial y_1}{\\partial z_j}\\frac{\\partial z_j}{\\partial W_{mn}}, \\dots, \\sum_{j=1}^{out} \\frac{\\partial y_{out}}{\\partial z_j}\\frac{\\partial z_j}{\\partial W_{mn}} \\right]^{\\top}\n",
    "\\end{align}\n",
    "\n",
    "The quantity in $\\mathbf{u}$ is easy. It is just the gradient of $l$ wrt $\\mathbf{y}$, that is, $\\nabla_{\\mathbf{y}}l$. For $\\mathbf{v}$, we know from the first section it is the Jacobian-vector prodict between $J_{\\mathbf{y} \\rightarrow z}$ and $\\left[\\frac{\\partial z_1}{\\partial W_{mn}}, \\dots, \\frac{\\partial z_{out}}{\\partial W_{mn}} \\right]^{\\top}$, thus\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{v} = \\begin{bmatrix}\n",
    "\\frac{\\partial y_1}{\\partial z_1} & \\dots & \\frac{\\partial y_1}{\\partial z_{out}} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial y_{out}}{\\partial z_1} & \\dots & \\frac{\\partial y_{out}}{\\partial z_{out}} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial z_1}{\\partial W_{mn}} \\\\ \n",
    "\\vdots \\\\\n",
    "\\frac{\\partial z_{out}}{\\partial W_{mn}}\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "We can solve for this expression by recalling that \n",
    "\n",
    "\\begin{equation}\n",
    "    z_k = x^{\\top}W[:, k] + b_k = \\sum_{j=1}^{in} x_j W_{jk} + b_k \\quad \\Rightarrow \\quad  \\frac{\\partial z_k}{W_{mn}} = \\begin{cases} \n",
    "    x_m & k = n            \\\\\n",
    "    0   & \\text{otherwise} \\\\\n",
    "   \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "This means the vector of partial derivatives $\\frac{\\partial z_k}{\\partial W_{mn}}$ is $0$ everywhere except at position $n$ where is equal to $x_m \\in \\mathbb{R}$. So, the expression for $\\mathbf{v}$ becomes\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{v} = \\begin{bmatrix}\n",
    "\\frac{\\partial y_1}{\\partial z_1} & \\dots & \\frac{\\partial y_1}{\\partial z_{out}} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial y_{out}}{\\partial z_1} & \\dots & \\frac{\\partial y_{out}}{\\partial z_{out}} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0 \\\\ \n",
    "\\vdots \\\\\n",
    "x_m \\\\\n",
    "\\vdots \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "= x_m\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial y_1}{\\partial z_n} \\\\ \n",
    "\\frac{\\partial y_2}{\\partial z_n} \\\\ \n",
    "\\vdots \\\\\n",
    "\\frac{\\partial y_{out}}{\\partial z_n} \\\\ \n",
    "\\end{bmatrix} \n",
    "= x_m J_{y \\rightarrow z}[:, n]\n",
    "\\end{equation} \n",
    "\n",
    "where $J_{y \\rightarrow z}[:, n]$ is the $n$th column of the Jacobian of $\\mathbf{y}$ wrt $\\mathbf{z}$. This allows us to arrive to our final expression for $\\frac{\\partial l}{\\partial W_{mn}}$, the $(m,n)$-th element of the gradient matrix $\\nabla_{W}l$.\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial l}{\\partial W_{mn}} = \\mathbf{u}^{\\top}\\mathbf{v} = x_m \\left(\\nabla_{\\mathbf{y}}l^{\\top} \\cdot J_{y \\rightarrow z}[:, n]\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Next, consider any two rows $m_1$ and $m_2$ from $\\nabla_{W}l$. These are given by\n",
    "\n",
    "\\begin{align}\n",
    "    \\nabla_{W}l[m1, :] &= x_{m_1}\\left(\\nabla_{\\mathbf{y}}l^{\\top} \\cdot J_{y \\rightarrow z}\\right) \\\\\n",
    "    \\nabla_{W}l[m2, :] &= x_{m_2}\\left(\\nabla_{\\mathbf{y}}l^{\\top} \\cdot J_{y \\rightarrow z}\\right)\n",
    "\\end{align}\n",
    "\n",
    "Observe they have the same expression, except from the scalar multipler $x_m$. Thus, we can generalize and express the full gradient matrix by collecting all such rows. To do so, first we recognize the expression in parenthesis to be the gradient of the loss with respect to the pre-activation vector $\\mathbf{z}$ (recall from first section that the gradient propagates via the Jacobian). So, define the column vector $\\nabla_{z}l \\in \\mathbb{R}^{out}$ as\n",
    "\n",
    "\\begin{equation}\n",
    "    \\nabla_{z}l = \\left(\\nabla_{\\mathbf{y}}l^{\\top} \\cdot J_{y \\rightarrow z}\\right)^{\\top} = J_{y \\rightarrow z}^{\\top} \\cdot \\nabla_{\\mathbf{y}}l\n",
    "\\end{equation}\n",
    "\n",
    "Now, using the fact that each row of $\\nabla_{W}l$ is a scalar multiple of the same vector $\\nabla_{z}l$, scaled by the corresponding element of $x$, we arrive at\n",
    "\n",
    "\\begin{equation}\n",
    "    \\nabla_{W}l = x \\cdot \\nabla_{z}l^{\\top}\n",
    "\\end{equation}\n",
    "\n",
    "Recall that $x \\in \\mathbb{R}^{in \\times 1}$ and $\\nabla_{z}l^{\\top} \\in \\mathbb{R}^{1 \\times out}$ are column and row vectors, respectively. Hence, the final result is an **outer product** between the two yielding a matrix of the same shape as $W$. \n",
    "\n",
    "\n",
    "## Gradient vector $\\nabla_{\\mathbf{b}}l$\n",
    "\n",
    "Here, we turn out attention to the bias and derive an expression for the gradient vector $\\nabla_{\\mathbf{b}}l$. Recall from the **Extending to more nested functions** section the fact that gradients propagate through intermediate functions via the Jacobian matrix. In our setting, the scalar loss $l$ depends on the bias vector $\\mathbf{b}$ through a sequence of transformations: $\\mathbf{b} \\rightarrow \\mathbf{z} \\rightarrow \\mathbf{y} \\rightarrow l$. This defines how the gradient must flow backwards using via succesive Jacobian matrices. In particular,\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{\\mathbf{b}}l = J_{\\mathbf{z} \\rightarrow \\mathbf{b}}^{\\top} J_{\\mathbf{y} \\rightarrow \\mathbf{z}}^{\\top} \\nabla_{\\mathbf{y}}l \n",
    "\\end{equation}\n",
    "\n",
    "Since $\\mathbf{z} = x^TW + b$, $J_{\\mathbf{z} \\rightarrow \\mathbf{b}} = \\mathbb{I}_{out}$. In words, each entry $z_i$ depends only on a single element $b_i$, thus the final Jacobian is just the identity matrix $\\mathbb{I}_{out}$. The expression for $\\nabla_{\\mathbf{b}}l$ becomes\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{\\mathbf{b}}l = \\mathbb{I}_{out} J_{\\mathbf{y} \\rightarrow \\mathbf{z}}^{\\top} \\nabla_{\\mathbf{y}}l  = J_{\\mathbf{y} \\rightarrow \\mathbf{z}}^{\\top} \\nabla_{\\mathbf{y}}l = \\nabla_{\\mathbf{z}}l\n",
    "\\end{equation}\n",
    "\n",
    "And we are done!\n",
    "\n",
    "\n",
    "## Implementing a Neural Network\n",
    "\n",
    "A common choice is to modularize each computational step into a `Layer` class that implements both a `forward` and `backward` method. For instance, the `forward` method of any `Layer` class will propagate the input `X` to the next layer. This next layer will then propagate such value even further (to the next layer) and so on. Notice the actual definition a `Layer` is rather arbitrarly. We could define a `Layer` to be a linear transformation (as we shall do) or, also valid, define it to be a linear transformation plus an activation function. Here, our design is motivated by how we mathematically defined each step of the whole computation.\n",
    "\n",
    "### Linear\n",
    "\n",
    "The linear layer applies an affine transformation to the input `X` and it is parametrized by two learnable arrays `W` and `b`; thus, the `forward` pass is just a what would expect: `X @ W + b`. The the `backward` pass must return the updated gradient of the loss function wrt to `W`. Earlier we concluded that \n",
    "\n",
    "\\begin{equation}\n",
    "    \\nabla_W l = J_{\\mathbf{z} \\rightarrow W}^{\\top} \\nabla_\\mathbf{z} l\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{z}$ hold the output of the linear transformation, i.e., $\\mathbf{z} = \\mathbf{x}^{\\top}W + \\mathbf{b}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
